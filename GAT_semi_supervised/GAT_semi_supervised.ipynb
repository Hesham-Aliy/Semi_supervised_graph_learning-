{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gr2DLtSzlOtC"
      },
      "outputs": [],
      "source": [
        "#! git clone https://github.com/gordicaleksa/pytorch-GAT.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install igraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fzec-szmn6a",
        "outputId": "c144f206-77c6-40e9-eea9-8eb1d9803725"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting igraph\n",
            "  Downloading igraph-0.10.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 6.7 MB/s \n",
            "\u001b[?25hCollecting texttable>=1.6.2\n",
            "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: texttable, igraph\n",
            "Successfully installed igraph-0.10.2 texttable-1.6.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I always like to structure my imports into Python's native libs,\n",
        "# stuff I installed via conda/pip and local file imports (but we don't have those here)\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "# Visualization related imports\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import igraph as ig\n",
        "\n",
        "# Main computation libraries\n",
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "\n",
        "# Deep learning related imports\n",
        "import torch"
      ],
      "metadata": {
        "id": "OI05bl2Lmle-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import enum\n",
        "\n",
        "\n",
        "# Supported datasets - only Cora in this notebook\n",
        "class DatasetType(enum.Enum):\n",
        "    CORA = 0\n",
        "\n",
        "    \n",
        "# Networkx is not precisely made with drawing as its main feature but I experimented with it a bit\n",
        "class GraphVisualizationTool(enum.Enum):\n",
        "    NETWORKX = 0,\n",
        "    IGRAPH = 1\n",
        "\n",
        "\n",
        "# We'll be dumping and reading the data from this directory\n",
        "DATA_DIR_PATH = os.path.join(os.getcwd(), 'data')\n",
        "CORA_PATH = os.path.join('./data/', 'cora')  # this is checked-in no need to make a directory\n",
        "\n",
        "#\n",
        "# Cora specific constants\n",
        "#\n",
        "\n",
        "# Thomas Kipf et al. first used this split in GCN paper and later Petar Veličković et al. in GAT paper\n",
        "CORA_TRAIN_RANGE = [0, 140]  # we're using the first 140 nodes as the training nodes\n",
        "CORA_VAL_RANGE = [140, 140+500]\n",
        "CORA_TEST_RANGE = [1708, 1708+1000]\n",
        "CORA_NUM_INPUT_FEATURES = 1433\n",
        "CORA_NUM_CLASSES = 7\n",
        "\n",
        "# Used whenever we need to visualzie points from different classes (t-SNE, CORA visualization)\n",
        "cora_label_to_color_map = {0: \"red\", 1: \"blue\", 2: \"green\", 3: \"orange\", 4: \"yellow\", 5: \"pink\", 6: \"gray\"}"
      ],
      "metadata": {
        "id": "3a8utdoUlTsi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First let's define these simple functions for loading/saving Pickle files - we need them for Cora\n",
        "\n",
        "# All Cora data is stored as pickle\n",
        "def pickle_read(path):\n",
        "    with open(path, 'rb') as file:\n",
        "        data = pickle.load(file)\n",
        "\n",
        "    return data\n",
        "\n",
        "def pickle_save(path, data):\n",
        "    with open(path, 'wb') as file:\n",
        "        pickle.dump(data, file, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "KSFbZHlrleJ3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_graph_data(training_config, device):\n",
        "    dataset_name = training_config['dataset_name'].lower()\n",
        "    should_visualize = training_config['should_visualize']\n",
        "\n",
        "    if dataset_name == DatasetType.CORA.name.lower():\n",
        "\n",
        "        # shape = (N, FIN), where N is the number of nodes and FIN is the number of input features\n",
        "        node_features_csr = pickle_read(os.path.join(CORA_PATH, 'node_features.csr'))\n",
        "        # shape = (N, 1)\n",
        "        node_labels_npy = pickle_read(os.path.join(CORA_PATH, 'node_labels.npy'))\n",
        "        # shape = (N, number of neighboring nodes) <- this is a dictionary not a matrix!\n",
        "        adjacency_list_dict = pickle_read(os.path.join(CORA_PATH, 'adjacency_list.dict'))\n",
        "\n",
        "        # Normalize the features (helps with training)\n",
        "        node_features_csr = normalize_features_sparse(node_features_csr)\n",
        "        num_of_nodes = len(node_labels_npy)\n",
        "\n",
        "        # shape = (2, E), where E is the number of edges, and 2 for source and target nodes. Basically edge index\n",
        "        # contains tuples of the format S->T, e.g. 0->3 means that node with id 0 points to a node with id 3.\n",
        "        topology = build_edge_index(adjacency_list_dict, num_of_nodes, add_self_edges=True)\n",
        "\n",
        "        # Note: topology is just a fancy way of naming the graph structure data \n",
        "        # (aside from edge index it could be in the form of an adjacency matrix)\n",
        "\n",
        "        if should_visualize:  # network analysis and graph drawing\n",
        "            plot_in_out_degree_distributions(topology, num_of_nodes, dataset_name)  # we'll define these in a second\n",
        "            visualize_graph(topology, node_labels_npy, dataset_name)\n",
        "\n",
        "        # Convert to dense PyTorch tensors\n",
        "\n",
        "        # Needs to be long int type because later functions like PyTorch's index_select expect it\n",
        "        topology = torch.tensor(topology, dtype=torch.long, device=device)\n",
        "        node_labels = torch.tensor(node_labels_npy, dtype=torch.long, device=device)  # Cross entropy expects a long int\n",
        "        node_features = torch.tensor(node_features_csr.todense(), device=device)\n",
        "\n",
        "        # Indices that help us extract nodes that belong to the train/val and test splits\n",
        "        train_indices = torch.arange(CORA_TRAIN_RANGE[0], CORA_TRAIN_RANGE[1], dtype=torch.long, device=device)\n",
        "        val_indices = torch.arange(CORA_VAL_RANGE[0], CORA_VAL_RANGE[1], dtype=torch.long, device=device)\n",
        "        test_indices = torch.arange(CORA_TEST_RANGE[0], CORA_TEST_RANGE[1], dtype=torch.long, device=device)\n",
        "\n",
        "        return node_features, node_labels, topology, train_indices, val_indices, test_indices\n",
        "    else:\n",
        "        raise Exception(f'{dataset_name} not yet supported.')"
      ],
      "metadata": {
        "id": "xyIOo-HrlaYq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_features_sparse(node_features_sparse):\n",
        "    assert sp.issparse(node_features_sparse), f'Expected a sparse matrix, got {node_features_sparse}.'\n",
        "\n",
        "    # Instead of dividing (like in normalize_features_dense()) we do multiplication with inverse sum of features.\n",
        "    # Modern hardware (GPUs, TPUs, ASICs) is optimized for fast matrix multiplications! ^^ (* >> /)\n",
        "    # shape = (N, FIN) -> (N, 1), where N number of nodes and FIN number of input features\n",
        "    node_features_sum = np.array(node_features_sparse.sum(-1))  # sum features for every node feature vector\n",
        "\n",
        "    # Make an inverse (remember * by 1/x is better (faster) then / by x)\n",
        "    # shape = (N, 1) -> (N)\n",
        "    node_features_inv_sum = np.power(node_features_sum, -1).squeeze()\n",
        "\n",
        "    # Again certain sums will be 0 so 1/0 will give us inf so we replace those by 1 which is a neutral element for mul\n",
        "    node_features_inv_sum[np.isinf(node_features_inv_sum)] = 1.\n",
        "\n",
        "    # Create a diagonal matrix whose values on the diagonal come from node_features_inv_sum\n",
        "    diagonal_inv_features_sum_matrix = sp.diags(node_features_inv_sum)\n",
        "\n",
        "    # We return the normalized features.\n",
        "    return diagonal_inv_features_sum_matrix.dot(node_features_sparse)"
      ],
      "metadata": {
        "id": "dVXA_3x9lje2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_edge_index(adjacency_list_dict, num_of_nodes, add_self_edges=True):\n",
        "    source_nodes_ids, target_nodes_ids = [], []\n",
        "    seen_edges = set()\n",
        "\n",
        "    for src_node, neighboring_nodes in adjacency_list_dict.items():\n",
        "        for trg_node in neighboring_nodes:\n",
        "            # if this edge hasn't been seen so far we add it to the edge index (coalescing - removing duplicates)\n",
        "            if (src_node, trg_node) not in seen_edges:  # it'd be easy to explicitly remove self-edges (Cora has none..)\n",
        "                source_nodes_ids.append(src_node)\n",
        "                target_nodes_ids.append(trg_node)\n",
        "\n",
        "                seen_edges.add((src_node, trg_node))\n",
        "\n",
        "    if add_self_edges:\n",
        "        source_nodes_ids.extend(np.arange(num_of_nodes))\n",
        "        target_nodes_ids.extend(np.arange(num_of_nodes))\n",
        "\n",
        "    # shape = (2, E), where E is the number of edges in the graph\n",
        "    edge_index = np.row_stack((source_nodes_ids, target_nodes_ids))\n",
        "\n",
        "    return edge_index"
      ],
      "metadata": {
        "id": "6PORejSglmnD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's just define dummy visualization functions for now - just to stop Python interpreter from complaining!\n",
        "# We'll define them in a moment, properly, I swear.\n",
        "import torch\n",
        "import pickle\n",
        "def plot_in_out_degree_distributions():\n",
        "    pass\n",
        "\n",
        "def visualize_graph():\n",
        "    pass\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # checking whether you have a GPU\n",
        "\n",
        "config = {\n",
        "    'dataset_name': DatasetType.CORA.name,\n",
        "    'should_visualize': False\n",
        "}\n",
        "\n",
        "node_features, node_labels, edge_index, train_indices, val_indices, test_indices = load_graph_data(config, device)\n",
        "\n",
        "print(node_features.shape, node_features.dtype)\n",
        "print(node_labels.shape, node_labels.dtype)\n",
        "print(edge_index.shape, edge_index.dtype)\n",
        "print(train_indices.shape, train_indices.dtype)\n",
        "print(val_indices.shape, val_indices.dtype)\n",
        "print(test_indices.shape, test_indices.dtype)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2y3QKeyal7Hh",
        "outputId": "63313083-b4c1-4411-f60a-d325be2fa726"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2708, 1433]) torch.float32\n",
            "torch.Size([2708]) torch.int64\n",
            "torch.Size([2, 13264]) torch.int64\n",
            "torch.Size([140]) torch.int64\n",
            "torch.Size([500]) torch.int64\n",
            "torch.Size([1000]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edge_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3ErW9_Ll9Yj",
        "outputId": "4accd873-0a9a-4e1b-a9b1-f6295805494d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   0,    0,    0,  ..., 2705, 2706, 2707],\n",
              "        [ 633, 1862, 2582,  ..., 2705, 2706, 2707]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    The most interesting and hardest implementation is implementation #3.\n",
        "    Imp1 and imp2 differ in subtle details but are basically the same thing.\n",
        "\n",
        "    So I'll focus on imp #3 in this notebook.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_of_layers, num_heads_per_layer, num_features_per_layer, add_skip_connection=True, bias=True,\n",
        "                 dropout=0.6, log_attention_weights=False):\n",
        "        super().__init__()\n",
        "        assert num_of_layers == len(num_heads_per_layer) == len(num_features_per_layer) - 1, f'Enter valid arch params.'\n",
        "\n",
        "        num_heads_per_layer = [1] + num_heads_per_layer  # trick - so that I can nicely create GAT layers below\n",
        "\n",
        "        gat_layers = []  # collect GAT layers\n",
        "        for i in range(num_of_layers):\n",
        "            layer = GATLayer(\n",
        "                num_in_features=num_features_per_layer[i] * num_heads_per_layer[i],  # consequence of concatenation\n",
        "                num_out_features=num_features_per_layer[i+1],\n",
        "                num_of_heads=num_heads_per_layer[i+1],\n",
        "                concat=True if i < num_of_layers - 1 else False,  # last GAT layer does mean avg, the others do concat\n",
        "                activation=nn.ELU() if i < num_of_layers - 1 else None,  # last layer just outputs raw scores\n",
        "                dropout_prob=dropout,\n",
        "                add_skip_connection=add_skip_connection,\n",
        "                bias=bias,\n",
        "                log_attention_weights=log_attention_weights\n",
        "            )\n",
        "            gat_layers.append(layer)\n",
        "\n",
        "        self.gat_net = nn.Sequential(\n",
        "            *gat_layers,\n",
        "        )\n",
        "\n",
        "    # data is just a (in_nodes_features, edge_index) tuple, I had to do it like this because of the nn.Sequential:\n",
        "    # https://discuss.pytorch.org/t/forward-takes-2-positional-arguments-but-3-were-given-for-nn-sqeuential-with-linear-layers/65698\n",
        "    def forward(self, data):\n",
        "        return self.gat_net(data)"
      ],
      "metadata": {
        "id": "P7e7ww6zmyg0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATLayer(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation #3 was inspired by PyTorch Geometric: https://github.com/rusty1s/pytorch_geometric\n",
        "\n",
        "    But, it's hopefully much more readable! (and of similar performance)\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    # We'll use these constants in many functions so just extracting them here as member fields\n",
        "    src_nodes_dim = 0  # position of source nodes in edge index\n",
        "    trg_nodes_dim = 1  # position of target nodes in edge index\n",
        "\n",
        "    # These may change in the inductive setting - leaving it like this for now (not future proof)\n",
        "    nodes_dim = 0      # node dimension (axis is maybe a more familiar term nodes_dim is the position of \"N\" in tensor)\n",
        "    head_dim = 1       # attention head dim\n",
        "\n",
        "    def __init__(self, num_in_features, num_out_features, num_of_heads, concat=True, activation=nn.ELU(),\n",
        "                 dropout_prob=0.6, add_skip_connection=True, bias=True, log_attention_weights=False):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_of_heads = num_of_heads\n",
        "        self.num_out_features = num_out_features\n",
        "        self.concat = concat  # whether we should concatenate or average the attention heads\n",
        "        self.add_skip_connection = add_skip_connection\n",
        "\n",
        "        #\n",
        "        # Trainable weights: linear projection matrix (denoted as \"W\" in the paper), attention target/source\n",
        "        # (denoted as \"a\" in the paper) and bias (not mentioned in the paper but present in the official GAT repo)\n",
        "        #\n",
        "\n",
        "        # You can treat this one matrix as num_of_heads independent W matrices\n",
        "        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
        "\n",
        "        # After we concatenate target node (node i) and source node (node j) we apply the \"additive\" scoring function\n",
        "        # which gives us un-normalized score \"e\". Here we split the \"a\" vector - but the semantics remain the same.\n",
        "        # Basically instead of doing [x, y] (concatenation, x/y are node feature vectors) and dot product with \"a\"\n",
        "        # we instead do a dot product between x and \"a_left\" and y and \"a_right\" and we sum them up\n",
        "        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
        "        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
        "\n",
        "        # Bias is definitely not crucial to GAT - feel free to experiment (I pinged the main author, Petar, on this one)\n",
        "        if bias and concat:\n",
        "            self.bias = nn.Parameter(torch.Tensor(num_of_heads * num_out_features))\n",
        "        elif bias and not concat:\n",
        "            self.bias = nn.Parameter(torch.Tensor(num_out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        if add_skip_connection:\n",
        "            self.skip_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
        "        else:\n",
        "            self.register_parameter('skip_proj', None)\n",
        "\n",
        "        #\n",
        "        # End of trainable weights\n",
        "        #\n",
        "\n",
        "        self.leakyReLU = nn.LeakyReLU(0.2)  # using 0.2 as in the paper, no need to expose every setting\n",
        "        self.activation = activation\n",
        "        # Probably not the nicest design but I use the same module in 3 locations, before/after features projection\n",
        "        # and for attention coefficients. Functionality-wise it's the same as using independent modules.\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "        self.log_attention_weights = log_attention_weights  # whether we should log the attention weights\n",
        "        self.attention_weights = None  # for later visualization purposes, I cache the weights here\n",
        "\n",
        "        self.init_params()\n",
        "        \n",
        "    def forward(self, data):\n",
        "        #\n",
        "        # Step 1: Linear Projection + regularization\n",
        "        #\n",
        "\n",
        "        in_nodes_features, edge_index = data  # unpack data\n",
        "        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n",
        "        assert edge_index.shape[0] == 2, f'Expected edge index with shape=(2,E) got {edge_index.shape}'\n",
        "\n",
        "        # shape = (N, FIN) where N - number of nodes in the graph, FIN - number of input features per node\n",
        "        # We apply the dropout to all of the input node features (as mentioned in the paper)\n",
        "        # Note: for Cora features are already super sparse so it's questionable how much this actually helps\n",
        "        in_nodes_features = self.dropout(in_nodes_features)\n",
        "\n",
        "        # shape = (N, FIN) * (FIN, NH*FOUT) -> (N, NH, FOUT) where NH - number of heads, FOUT - num of output features\n",
        "        # We project the input node features into NH independent output features (one for each attention head)\n",
        "        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
        "\n",
        "        nodes_features_proj = self.dropout(nodes_features_proj)  # in the official GAT imp they did dropout here as well\n",
        "\n",
        "        #\n",
        "        # Step 2: Edge attention calculation\n",
        "        #\n",
        "\n",
        "        # Apply the scoring function (* represents element-wise (a.k.a. Hadamard) product)\n",
        "        # shape = (N, NH, FOUT) * (1, NH, FOUT) -> (N, NH, 1) -> (N, NH) because sum squeezes the last dimension\n",
        "        # Optimization note: torch.sum() is as performant as .sum() in my experiments\n",
        "        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n",
        "        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n",
        "\n",
        "        # We simply copy (lift) the scores for source/target nodes based on the edge index. Instead of preparing all\n",
        "        # the possible combinations of scores we just prepare those that will actually be used and those are defined\n",
        "        # by the edge index.\n",
        "        # scores shape = (E, NH), nodes_features_proj_lifted shape = (E, NH, FOUT), E - number of edges in the graph\n",
        "        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n",
        "        scores_per_edge = self.leakyReLU(scores_source_lifted + scores_target_lifted)\n",
        "\n",
        "        # shape = (E, NH, 1)\n",
        "        attentions_per_edge = self.neighborhood_aware_softmax(scores_per_edge, edge_index[self.trg_nodes_dim], num_of_nodes)\n",
        "        # Add stochasticity to neighborhood aggregation\n",
        "        attentions_per_edge = self.dropout(attentions_per_edge)\n",
        "\n",
        "        #\n",
        "        # Step 3: Neighborhood aggregation\n",
        "        #\n",
        "\n",
        "        # Element-wise (aka Hadamard) product. Operator * does the same thing as torch.mul\n",
        "        # shape = (E, NH, FOUT) * (E, NH, 1) -> (E, NH, FOUT), 1 gets broadcast into FOUT\n",
        "        nodes_features_proj_lifted_weighted = nodes_features_proj_lifted * attentions_per_edge\n",
        "\n",
        "        # This part sums up weighted and projected neighborhood feature vectors for every target node\n",
        "        # shape = (N, NH, FOUT)\n",
        "        out_nodes_features = self.aggregate_neighbors(nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes)\n",
        "\n",
        "        #\n",
        "        # Step 4: Residual/skip connections, concat and bias\n",
        "        #\n",
        "\n",
        "        out_nodes_features = self.skip_concat_bias(attentions_per_edge, in_nodes_features, out_nodes_features)\n",
        "        return (out_nodes_features, edge_index)\n",
        "\n",
        "    #\n",
        "    # Helper functions (without comments there is very little code so don't be scared!)\n",
        "    #\n",
        "\n",
        "    def neighborhood_aware_softmax(self, scores_per_edge, trg_index, num_of_nodes):\n",
        "        \"\"\"\n",
        "        As the fn name suggest it does softmax over the neighborhoods. Example: say we have 5 nodes in a graph.\n",
        "        Two of them 1, 2 are connected to node 3. If we want to calculate the representation for node 3 we should take\n",
        "        into account feature vectors of 1, 2 and 3 itself. Since we have scores for edges 1-3, 2-3 and 3-3\n",
        "        in scores_per_edge variable, this function will calculate attention scores like this: 1-3/(1-3+2-3+3-3)\n",
        "        (where 1-3 is overloaded notation it represents the edge 1-3 and its (exp) score) and similarly for 2-3 and 3-3\n",
        "         i.e. for this neighborhood we don't care about other edge scores that include nodes 4 and 5.\n",
        "\n",
        "        Note:\n",
        "        Subtracting the max value from logits doesn't change the end result but it improves the numerical stability\n",
        "        and it's a fairly common \"trick\" used in pretty much every deep learning framework.\n",
        "        Check out this link for more details:\n",
        "\n",
        "        https://stats.stackexchange.com/questions/338285/how-does-the-subtraction-of-the-logit-maximum-improve-learning\n",
        "\n",
        "        \"\"\"\n",
        "        # Calculate the numerator. Make logits <= 0 so that e^logit <= 1 (this will improve the numerical stability)\n",
        "        scores_per_edge = scores_per_edge - scores_per_edge.max()\n",
        "        exp_scores_per_edge = scores_per_edge.exp()  # softmax\n",
        "\n",
        "        # Calculate the denominator. shape = (E, NH)\n",
        "        neigborhood_aware_denominator = self.sum_edge_scores_neighborhood_aware(exp_scores_per_edge, trg_index, num_of_nodes)\n",
        "\n",
        "        # 1e-16 is theoretically not needed but is only there for numerical stability (avoid div by 0) - due to the\n",
        "        # possibility of the computer rounding a very small number all the way to 0.\n",
        "        attentions_per_edge = exp_scores_per_edge / (neigborhood_aware_denominator + 1e-16)\n",
        "\n",
        "        # shape = (E, NH) -> (E, NH, 1) so that we can do element-wise multiplication with projected node features\n",
        "        return attentions_per_edge.unsqueeze(-1)\n",
        "\n",
        "    def sum_edge_scores_neighborhood_aware(self, exp_scores_per_edge, trg_index, num_of_nodes):\n",
        "        # The shape must be the same as in exp_scores_per_edge (required by scatter_add_) i.e. from E -> (E, NH)\n",
        "        trg_index_broadcasted = self.explicit_broadcast(trg_index, exp_scores_per_edge)\n",
        "\n",
        "        # shape = (N, NH), where N is the number of nodes and NH the number of attention heads\n",
        "        size = list(exp_scores_per_edge.shape)  # convert to list otherwise assignment is not possible\n",
        "        size[self.nodes_dim] = num_of_nodes\n",
        "        neighborhood_sums = torch.zeros(size, dtype=exp_scores_per_edge.dtype, device=exp_scores_per_edge.device)\n",
        "\n",
        "        # position i will contain a sum of exp scores of all the nodes that point to the node i (as dictated by the\n",
        "        # target index)\n",
        "        neighborhood_sums.scatter_add_(self.nodes_dim, trg_index_broadcasted, exp_scores_per_edge)\n",
        "\n",
        "        # Expand again so that we can use it as a softmax denominator. e.g. node i's sum will be copied to\n",
        "        # all the locations where the source nodes pointed to i (as dictated by the target index)\n",
        "        # shape = (N, NH) -> (E, NH)\n",
        "        return neighborhood_sums.index_select(self.nodes_dim, trg_index)\n",
        "\n",
        "    def aggregate_neighbors(self, nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes):\n",
        "        size = list(nodes_features_proj_lifted_weighted.shape)  # convert to list otherwise assignment is not possible\n",
        "        size[self.nodes_dim] = num_of_nodes  # shape = (N, NH, FOUT)\n",
        "        out_nodes_features = torch.zeros(size, dtype=in_nodes_features.dtype, device=in_nodes_features.device)\n",
        "\n",
        "        # shape = (E) -> (E, NH, FOUT)\n",
        "        trg_index_broadcasted = self.explicit_broadcast(edge_index[self.trg_nodes_dim], nodes_features_proj_lifted_weighted)\n",
        "        # aggregation step - we accumulate projected, weighted node features for all the attention heads\n",
        "        # shape = (E, NH, FOUT) -> (N, NH, FOUT)\n",
        "        out_nodes_features.scatter_add_(self.nodes_dim, trg_index_broadcasted, nodes_features_proj_lifted_weighted)\n",
        "\n",
        "        return out_nodes_features\n",
        "\n",
        "    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n",
        "        \"\"\"\n",
        "        Lifts i.e. duplicates certain vectors depending on the edge index.\n",
        "        One of the tensor dims goes from N -> E (that's where the \"lift\" comes from).\n",
        "\n",
        "        \"\"\"\n",
        "        src_nodes_index = edge_index[self.src_nodes_dim]\n",
        "        trg_nodes_index = edge_index[self.trg_nodes_dim]\n",
        "\n",
        "        # Using index_select is faster than \"normal\" indexing (scores_source[src_nodes_index]) in PyTorch!\n",
        "        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n",
        "        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n",
        "        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n",
        "\n",
        "        return scores_source, scores_target, nodes_features_matrix_proj_lifted\n",
        "\n",
        "    def explicit_broadcast(self, this, other):\n",
        "        # Append singleton dimensions until this.dim() == other.dim()\n",
        "        for _ in range(this.dim(), other.dim()):\n",
        "            this = this.unsqueeze(-1)\n",
        "\n",
        "        # Explicitly expand so that shapes are the same\n",
        "        return this.expand_as(other)\n",
        "\n",
        "    def init_params(self):\n",
        "        \"\"\"\n",
        "        The reason we're using Glorot (aka Xavier uniform) initialization is because it's a default TF initialization:\n",
        "            https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow\n",
        "\n",
        "        The original repo was developed in TensorFlow (TF) and they used the default initialization.\n",
        "        Feel free to experiment - there may be better initializations depending on your problem.\n",
        "\n",
        "        \"\"\"\n",
        "        nn.init.xavier_uniform_(self.linear_proj.weight)\n",
        "        nn.init.xavier_uniform_(self.scoring_fn_target)\n",
        "        nn.init.xavier_uniform_(self.scoring_fn_source)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            torch.nn.init.zeros_(self.bias)\n",
        "\n",
        "    def skip_concat_bias(self, attention_coefficients, in_nodes_features, out_nodes_features):\n",
        "        if self.log_attention_weights:  # potentially log for later visualization in playground.py\n",
        "            self.attention_weights = attention_coefficients\n",
        "\n",
        "        if self.add_skip_connection:  # add skip or residual connection\n",
        "            if out_nodes_features.shape[-1] == in_nodes_features.shape[-1]:  # if FIN == FOUT\n",
        "                # unsqueeze does this: (N, FIN) -> (N, 1, FIN), out features are (N, NH, FOUT) so 1 gets broadcast to NH\n",
        "                # thus we're basically copying input vectors NH times and adding to processed vectors\n",
        "                out_nodes_features += in_nodes_features.unsqueeze(1)\n",
        "            else:\n",
        "                # FIN != FOUT so we need to project input feature vectors into dimension that can be added to output\n",
        "                # feature vectors. skip_proj adds lots of additional capacity which may cause overfitting.\n",
        "                out_nodes_features += self.skip_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
        "\n",
        "        if self.concat:\n",
        "            # shape = (N, NH, FOUT) -> (N, NH*FOUT)\n",
        "            out_nodes_features = out_nodes_features.view(-1, self.num_of_heads * self.num_out_features)\n",
        "        else:\n",
        "            # shape = (N, NH, FOUT) -> (N, FOUT)\n",
        "            out_nodes_features = out_nodes_features.mean(dim=self.head_dim)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            out_nodes_features += self.bias\n",
        "\n",
        "        return out_nodes_features if self.activation is None else self.activation(out_nodes_features)"
      ],
      "metadata": {
        "id": "P9wg0-mvsYcC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "# 3 different model training/eval phases used in train.py\n",
        "class LoopPhase(enum.Enum):\n",
        "    TRAIN = 0,\n",
        "    VAL = 1,\n",
        "    TEST = 2\n",
        "\n",
        "    \n",
        "writer = SummaryWriter()  # (tensorboard) writer will output to ./runs/ directory by default\n",
        "\n",
        "\n",
        "# Global vars used for early stopping. After some number of epochs (as defined by the patience_period var) without any\n",
        "# improvement on the validation dataset (measured via accuracy metric), we'll break out from the training loop.\n",
        "BEST_VAL_ACC = 0\n",
        "BEST_VAL_LOSS = 0\n",
        "PATIENCE_CNT = 0\n",
        "\n",
        "BINARIES_PATH = os.path.join(os.getcwd(), 'models', 'binaries')\n",
        "CHECKPOINTS_PATH = os.path.join(os.getcwd(), 'models', 'checkpoints')\n",
        "\n",
        "# Make sure these exist as the rest of the code assumes it\n",
        "os.makedirs(BINARIES_PATH, exist_ok=True)\n",
        "os.makedirs(CHECKPOINTS_PATH, exist_ok=True)"
      ],
      "metadata": {
        "id": "7ylQFSK2saUf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gitpython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKgmzzv2tB0A",
        "outputId": "be57902a-dd83-489c-d6f1-768f8c57651c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gitpython\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython) (4.1.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.10 gitpython-3.1.29 smmap-5.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import git\n",
        "import re  # regex\n",
        "\n",
        "\n",
        "def get_training_state(training_config, model):\n",
        "    training_state = {\n",
        "        \"commit_hash\": git.Repo(search_parent_directories=True).head.object.hexsha,\n",
        "\n",
        "        # Training details\n",
        "        \"dataset_name\": training_config['dataset_name'],\n",
        "        \"num_of_epochs\": training_config['num_of_epochs'],\n",
        "        \"test_acc\": training_config['test_acc'],\n",
        "\n",
        "        # Model structure\n",
        "        \"num_of_layers\": training_config['num_of_layers'],\n",
        "        \"num_heads_per_layer\": training_config['num_heads_per_layer'],\n",
        "        \"num_features_per_layer\": training_config['num_features_per_layer'],\n",
        "        \"add_skip_connection\": training_config['add_skip_connection'],\n",
        "        \"bias\": training_config['bias'],\n",
        "        \"dropout\": training_config['dropout'],\n",
        "\n",
        "        # Model state\n",
        "        \"state_dict\": model.state_dict()\n",
        "    }\n",
        "\n",
        "    return training_state\n",
        "\n",
        "\n",
        "def print_model_metadata(training_state):\n",
        "    header = f'\\n{\"*\"*5} Model training metadata: {\"*\"*5}'\n",
        "    print(header)\n",
        "\n",
        "    for key, value in training_state.items():\n",
        "        if key != 'state_dict':  # don't print state_dict just a bunch of numbers...\n",
        "            print(f'{key}: {value}')\n",
        "    print(f'{\"*\" * len(header)}\\n')\n",
        "\n",
        "\n",
        "# This one makes sure we don't overwrite the valuable model binaries (feel free to ignore - not crucial to GAT method)\n",
        "def get_available_binary_name():\n",
        "    prefix = 'gat'\n",
        "\n",
        "    def valid_binary_name(binary_name):\n",
        "        # First time you see raw f-string? Don't worry the only trick is to double the brackets.\n",
        "        pattern = re.compile(rf'{prefix}_[0-9]{{6}}\\.pth')\n",
        "        return re.fullmatch(pattern, binary_name) is not None\n",
        "\n",
        "    # Just list the existing binaries so that we don't overwrite them but write to a new one\n",
        "    valid_binary_names = list(filter(valid_binary_name, os.listdir(BINARIES_PATH)))\n",
        "    if len(valid_binary_names) > 0:\n",
        "        last_binary_name = sorted(valid_binary_names)[-1]\n",
        "        new_suffix = int(last_binary_name.split('.')[0][-6:]) + 1  # increment by 1\n",
        "        return f'{prefix}_{str(new_suffix).zfill(6)}.pth'\n",
        "    else:\n",
        "        return f'{prefix}_000000.pth'"
      ],
      "metadata": {
        "id": "4mmRFCqps0I3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "\n",
        "def get_training_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Training related\n",
        "    parser.add_argument(\"--num_of_epochs\", type=int, help=\"number of training epochs\", default=10000)\n",
        "    parser.add_argument(\"--patience_period\", type=int, help=\"number of epochs with no improvement on val before terminating\", default=1000)\n",
        "    parser.add_argument(\"--lr\", type=float, help=\"model learning rate\", default=5e-3)\n",
        "    parser.add_argument(\"--weight_decay\", type=float, help=\"L2 regularization on model weights\", default=5e-4)\n",
        "    parser.add_argument(\"--should_test\", type=bool, help='should test the model on the test dataset?', default=True)\n",
        "\n",
        "    # Dataset related\n",
        "    parser.add_argument(\"--dataset_name\", choices=[el.name for el in DatasetType], help='dataset to use for training', default=DatasetType.CORA.name)\n",
        "    parser.add_argument(\"--should_visualize\", type=bool, help='should visualize the dataset?', default=False)\n",
        "\n",
        "    # Logging/debugging/checkpoint related (helps a lot with experimentation)\n",
        "    parser.add_argument(\"--enable_tensorboard\", type=bool, help=\"enable tensorboard logging\", default=False)\n",
        "    parser.add_argument(\"--console_log_freq\", type=int, help=\"log to output console (epoch) freq (None for no logging)\", default=100)\n",
        "    parser.add_argument(\"--checkpoint_freq\", type=int, help=\"checkpoint model saving (epoch) freq (None for no logging)\", default=1000)\n",
        "    args, unknown = parser.parse_known_args(\"\")\n",
        "\n",
        "    # Model architecture related - this is the architecture as defined in the official paper (for Cora classification)\n",
        "    gat_config = {\n",
        "        \"num_of_layers\": 2,  # GNNs, contrary to CNNs, are often shallow (it ultimately depends on the graph properties)\n",
        "        \"num_heads_per_layer\": [8, 1],\n",
        "        \"num_features_per_layer\": [CORA_NUM_INPUT_FEATURES, 8, CORA_NUM_CLASSES],\n",
        "        \"add_skip_connection\": False,  # hurts perf on Cora\n",
        "        \"bias\": True,  # result is not so sensitive to bias\n",
        "        \"dropout\": 0.6,  # result is sensitive to dropout\n",
        "    }\n",
        "\n",
        "    # Wrapping training configuration into a dictionary\n",
        "    training_config = dict()\n",
        "    for arg in vars(args):\n",
        "        training_config[arg] = getattr(args, arg)\n",
        "\n",
        "    # Add additional config information\n",
        "    training_config.update(gat_config)\n",
        "\n",
        "    return training_config"
      ],
      "metadata": {
        "id": "TEUy9Zkds8qv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config= get_training_args()"
      ],
      "metadata": {
        "id": "VqurySJXuPfx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # checking whether you have a GPU, I hope so!\n",
        "\n",
        "# Step 1: load the graph data\n",
        "node_features, node_labels, edge_index, train_indices, val_indices, test_indices = load_graph_data(config, device)\n",
        "\n",
        "# Step 2: prepare the model\n",
        "gat = GAT(\n",
        "    num_of_layers=config['num_of_layers'],\n",
        "    num_heads_per_layer=config['num_heads_per_layer'],\n",
        "    num_features_per_layer=config['num_features_per_layer'],\n",
        "    add_skip_connection=config['add_skip_connection'],\n",
        "    bias=config['bias'],\n",
        "    dropout=config['dropout'],\n",
        "    log_attention_weights=False  # no need to store attentions, used only in playground.py while visualizing\n",
        ").to(device)\n",
        "\n",
        "# Step 3: Prepare other training related utilities (loss & optimizer and decorator function)\n",
        "loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
        "optimizer = Adam(gat.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])"
      ],
      "metadata": {
        "id": "eiz7rU0HtQy9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_dim = 0  # this will likely change as soon as I add an inductive example (Cora is transductive)\n",
        "\n",
        "\n",
        "train_labels = node_labels.index_select(node_dim, train_indices)\n",
        "val_labels = node_labels.index_select(node_dim, val_indices)\n",
        "test_labels = node_labels.index_select(node_dim, test_indices)\n",
        "\n",
        "# node_features shape = (N, FIN), edge_index shape = (2, E)\n",
        "graph_data = (node_features, edge_index)  # I pack data into tuples because GAT uses nn.Sequential which requires it\n",
        "\n",
        "def get_node_indices(phase):\n",
        "    if phase == LoopPhase.TRAIN:\n",
        "        return train_indices\n",
        "    elif phase == LoopPhase.VAL:\n",
        "        return val_indices\n",
        "    else:\n",
        "        return test_indices\n",
        "\n",
        "def get_node_labels(phase):\n",
        "    if phase == LoopPhase.TRAIN:\n",
        "        return train_labels\n",
        "    elif phase == LoopPhase.VAL:\n",
        "        return val_labels\n",
        "    else:\n",
        "        return test_labels"
      ],
      "metadata": {
        "id": "4DSYKs9FuKaa"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phase=LoopPhase.TRAIN\n"
      ],
      "metadata": {
        "id": "IllfZeDawlQb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "eCMe7ncaNN-6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_loop(phase, epoch=0):\n",
        "        global BEST_VAL_ACC, BEST_VAL_LOSS, PATIENCE_CNT, writer\n",
        "\n",
        "        # Certain modules behave differently depending on whether we're training the model or not.\n",
        "        # e.g. nn.Dropout - we only want to drop model weights during the training.\n",
        "        patience_period =  config['patience_period']\n",
        "      \n",
        "        time_start =  time.time()\n",
        "        \n",
        "        if phase == LoopPhase.TRAIN:\n",
        "            gat.train()\n",
        "        else:\n",
        "            gat.eval()\n",
        "\n",
        "        node_indices = get_node_indices(phase)\n",
        "        gt_node_labels = get_node_labels(phase)  # gt stands for ground truth\n",
        "\n",
        "        # Do a forwards pass and extract only the relevant node scores (train/val or test ones)\n",
        "        # Note: [0] just extracts the node_features part of the data (index 1 contains the edge_index)\n",
        "        # shape = (N, C) where N is the number of nodes in the split (train/val/test) and C is the number of classes\n",
        "        nodes_unnormalized_scores = gat(graph_data)[0].index_select(node_dim, node_indices)\n",
        "\n",
        "        # Example: let's take an output for a single node on Cora - it's a vector of size 7 and it contains unnormalized\n",
        "        # scores like: V = [-1.393,  3.0765, -2.4445,  9.6219,  2.1658, -5.5243, -4.6247]\n",
        "        # What PyTorch's cross entropy loss does is for every such vector it first applies a softmax, and so we'll\n",
        "        # have the V transformed into: [1.6421e-05, 1.4338e-03, 5.7378e-06, 0.99797, 5.7673e-04, 2.6376e-07, 6.4848e-07]\n",
        "        # secondly, whatever the correct class is (say it's 3), it will then take the element at position 3,\n",
        "        # 0.99797 in this case, and the loss will be -log(0.99797). It does this for every node and applies a mean.\n",
        "        # You can see that as the probability of the correct class for most nodes approaches 1 we get to 0 loss! <3\n",
        "        loss = loss_fn(nodes_unnormalized_scores, gt_node_labels)\n",
        "\n",
        "        if phase == LoopPhase.TRAIN:\n",
        "            optimizer.zero_grad()  # clean the trainable weights gradients in the computational graph (.grad fields)\n",
        "            loss.backward()  # compute the gradients for every trainable weight in the computational graph\n",
        "            optimizer.step()  # apply the gradients to weights\n",
        "\n",
        "        # Finds the index of maximum (unnormalized) score for every node and that's the class prediction for that node.\n",
        "        # Compare those to true (ground truth) labels and find the fraction of correct predictions -> accuracy metric.\n",
        "        class_predictions = torch.argmax(nodes_unnormalized_scores, dim=-1)\n",
        "        accuracy = torch.sum(torch.eq(class_predictions, gt_node_labels).long()).item() / len(gt_node_labels)\n",
        "        \n",
        "        print(f'Train accuracy = {accuracy}')\n",
        "        print(f'Train_ Loss =  {loss}')\n",
        "\n",
        "        #\n",
        "        # Logging\n",
        "        #\n",
        "\n",
        "        if phase == LoopPhase.TRAIN:\n",
        "            # Log metrics\n",
        "            if config['enable_tensorboard']:\n",
        "                writer.add_scalar('training_loss', loss.item(), epoch)\n",
        "                writer.add_scalar('training_acc', accuracy, epoch)\n",
        "\n",
        "            # Save model checkpoint\n",
        "            if config['checkpoint_freq'] is not None and (epoch + 1) % config['checkpoint_freq'] == 0:\n",
        "                ckpt_model_name = f\"gat_ckpt_epoch_{epoch + 1}.pth\"\n",
        "                config['test_acc'] = -1\n",
        "                torch.save(gat.state_dict(), '/content/GAT_semi.pt')\n",
        "\n",
        "        elif phase == LoopPhase.VAL:\n",
        "            # Log metrics\n",
        "            if config['enable_tensorboard']:\n",
        "                writer.add_scalar('val_loss', loss.item(), epoch)\n",
        "                writer.add_scalar('val_acc', accuracy, epoch)\n",
        "\n",
        "            # Log to console\n",
        "            if config['console_log_freq'] is not None and epoch % config['console_log_freq'] == 0:\n",
        "                print(f'GAT training: time elapsed= {(time.time() - time_start):.2f} [s] | epoch={epoch + 1} | val acc={accuracy}')\n",
        "\n",
        "            # The \"patience\" logic - should we break out from the training loop? If either validation acc keeps going up\n",
        "            # or the val loss keeps going down we won't stop\n",
        "            if accuracy > BEST_VAL_ACC or loss.item() < BEST_VAL_LOSS:\n",
        "                BEST_VAL_ACC = max(accuracy, BEST_VAL_ACC)  # keep track of the best validation accuracy so far\n",
        "                BEST_VAL_LOSS = min(loss.item(), BEST_VAL_LOSS)\n",
        "                PATIENCE_CNT = 0  # reset the counter every time we encounter new best accuracy\n",
        "            else:\n",
        "                PATIENCE_CNT += 1  # otherwise keep counting\n",
        "        else:\n",
        "            return accuracy"
      ],
      "metadata": {
        "id": "oAnU8eDPxYw0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 4: Start the training procedure\n",
        "for epoch in range(config['num_of_epochs']):\n",
        "    print(f'Eopch = {epoch}')\n",
        "    # Training loop\n",
        "    main_loop(phase=LoopPhase.TRAIN, epoch=epoch)\n",
        "\n",
        "    # Validation loop\n",
        "    with torch.no_grad():\n",
        "      main_loop(phase=LoopPhase.VAL, epoch=epoch)\n",
        "       # break out from the training loop\n",
        "\n",
        "# Step 5: Potentially test your model\n",
        "# Don't overfit to the test dataset - only when you've fine-tuned your model on the validation dataset should you\n",
        "# report your final loss and accuracy on the test dataset. Friends don't let friends overfit to the test data. <3\n",
        "if config['should_test']:\n",
        "    test_acc = main_loop(phase=LoopPhase.TEST)\n",
        "    config['test_acc'] = test_acc\n",
        "    print(f'Test accuracy = {test_acc}')\n",
        "else:\n",
        "    config['test_acc'] = -1\n",
        "\n",
        "# Save the latest GAT in the binaries directory\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZxys3SkLO4a",
        "outputId": "a9b07849-fb92-4979-a8a1-e91bb8498d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eopch = 0\n",
            "Train accuracy = 0.12857142857142856\n",
            "Train_ Loss =  1.9576952457427979\n",
            "Train accuracy = 0.088\n",
            "Train_ Loss =  1.9453386068344116\n",
            "GAT training: time elapsed= 0.02 [s] | epoch=1 | val acc=0.088\n",
            "Eopch = 1\n",
            "Train accuracy = 0.19285714285714287\n",
            "Train_ Loss =  1.947758436203003\n",
            "Train accuracy = 0.112\n",
            "Train_ Loss =  1.9454888105392456\n",
            "Eopch = 2\n",
            "Train accuracy = 0.11428571428571428\n",
            "Train_ Loss =  1.9463356733322144\n",
            "Train accuracy = 0.114\n",
            "Train_ Loss =  1.94460928440094\n",
            "Eopch = 3\n",
            "Train accuracy = 0.12142857142857143\n",
            "Train_ Loss =  1.946467638015747\n",
            "Train accuracy = 0.12\n",
            "Train_ Loss =  1.9446289539337158\n",
            "Eopch = 4\n",
            "Train accuracy = 0.21428571428571427\n",
            "Train_ Loss =  1.9387530088424683\n",
            "Train accuracy = 0.116\n",
            "Train_ Loss =  1.9422894716262817\n",
            "Eopch = 5\n",
            "Train accuracy = 0.21428571428571427\n",
            "Train_ Loss =  1.9354432821273804\n",
            "Train accuracy = 0.206\n",
            "Train_ Loss =  1.9382432699203491\n",
            "Eopch = 6\n",
            "Train accuracy = 0.22857142857142856\n",
            "Train_ Loss =  1.9251750707626343\n",
            "Train accuracy = 0.356\n",
            "Train_ Loss =  1.9350535869598389\n",
            "Eopch = 7\n",
            "Train accuracy = 0.19285714285714287\n",
            "Train_ Loss =  1.9329591989517212\n",
            "Train accuracy = 0.442\n",
            "Train_ Loss =  1.9327410459518433\n",
            "Eopch = 8\n",
            "Train accuracy = 0.29285714285714287\n",
            "Train_ Loss =  1.9184284210205078\n",
            "Train accuracy = 0.418\n",
            "Train_ Loss =  1.9309406280517578\n",
            "Eopch = 9\n",
            "Train accuracy = 0.22142857142857142\n",
            "Train_ Loss =  1.9246536493301392\n",
            "Train accuracy = 0.384\n",
            "Train_ Loss =  1.928935170173645\n",
            "Eopch = 10\n",
            "Train accuracy = 0.25\n",
            "Train_ Loss =  1.9203977584838867\n",
            "Train accuracy = 0.392\n",
            "Train_ Loss =  1.9269068241119385\n",
            "Eopch = 11\n",
            "Train accuracy = 0.2642857142857143\n",
            "Train_ Loss =  1.9188008308410645\n",
            "Train accuracy = 0.426\n",
            "Train_ Loss =  1.9241621494293213\n",
            "Eopch = 12\n",
            "Train accuracy = 0.22142857142857142\n",
            "Train_ Loss =  1.9154810905456543\n",
            "Train accuracy = 0.496\n",
            "Train_ Loss =  1.9215470552444458\n",
            "Eopch = 13\n",
            "Train accuracy = 0.29285714285714287\n",
            "Train_ Loss =  1.902748703956604\n",
            "Train accuracy = 0.562\n",
            "Train_ Loss =  1.9187787771224976\n",
            "Eopch = 14\n",
            "Train accuracy = 0.30714285714285716\n",
            "Train_ Loss =  1.902769923210144\n",
            "Train accuracy = 0.592\n",
            "Train_ Loss =  1.915768027305603\n",
            "Eopch = 15\n",
            "Train accuracy = 0.39285714285714285\n",
            "Train_ Loss =  1.8828067779541016\n",
            "Train accuracy = 0.57\n",
            "Train_ Loss =  1.912724256515503\n",
            "Eopch = 16\n",
            "Train accuracy = 0.2785714285714286\n",
            "Train_ Loss =  1.905760407447815\n",
            "Train accuracy = 0.58\n",
            "Train_ Loss =  1.9091708660125732\n",
            "Eopch = 17\n",
            "Train accuracy = 0.34285714285714286\n",
            "Train_ Loss =  1.9004364013671875\n",
            "Train accuracy = 0.588\n",
            "Train_ Loss =  1.9051861763000488\n",
            "Eopch = 18\n",
            "Train accuracy = 0.2357142857142857\n",
            "Train_ Loss =  1.9060115814208984\n",
            "Train accuracy = 0.562\n",
            "Train_ Loss =  1.9001699686050415\n",
            "Eopch = 19\n",
            "Train accuracy = 0.35\n",
            "Train_ Loss =  1.8813594579696655\n",
            "Train accuracy = 0.552\n",
            "Train_ Loss =  1.8958224058151245\n",
            "Eopch = 20\n",
            "Train accuracy = 0.29285714285714287\n",
            "Train_ Loss =  1.8976409435272217\n",
            "Train accuracy = 0.548\n",
            "Train_ Loss =  1.8918251991271973\n",
            "Eopch = 21\n",
            "Train accuracy = 0.35\n",
            "Train_ Loss =  1.8768783807754517\n",
            "Train accuracy = 0.564\n",
            "Train_ Loss =  1.889424443244934\n",
            "Eopch = 22\n",
            "Train accuracy = 0.35\n",
            "Train_ Loss =  1.8781803846359253\n",
            "Train accuracy = 0.572\n",
            "Train_ Loss =  1.8876793384552002\n",
            "Eopch = 23\n",
            "Train accuracy = 0.36428571428571427\n",
            "Train_ Loss =  1.8691000938415527\n",
            "Train accuracy = 0.574\n",
            "Train_ Loss =  1.8870031833648682\n",
            "Eopch = 24\n",
            "Train accuracy = 0.4357142857142857\n",
            "Train_ Loss =  1.8604997396469116\n",
            "Train accuracy = 0.59\n",
            "Train_ Loss =  1.8870247602462769\n",
            "Eopch = 25\n",
            "Train accuracy = 0.34285714285714286\n",
            "Train_ Loss =  1.8633252382278442\n",
            "Train accuracy = 0.622\n",
            "Train_ Loss =  1.8871374130249023\n",
            "Eopch = 26\n",
            "Train accuracy = 0.4714285714285714\n",
            "Train_ Loss =  1.832759976387024\n",
            "Train accuracy = 0.634\n",
            "Train_ Loss =  1.8870292901992798\n",
            "Eopch = 27\n",
            "Train accuracy = 0.37142857142857144\n",
            "Train_ Loss =  1.869600772857666\n",
            "Train accuracy = 0.634\n",
            "Train_ Loss =  1.8865922689437866\n",
            "Eopch = 28\n",
            "Train accuracy = 0.4142857142857143\n",
            "Train_ Loss =  1.860625147819519\n",
            "Train accuracy = 0.628\n",
            "Train_ Loss =  1.8859601020812988\n",
            "Eopch = 29\n",
            "Train accuracy = 0.4142857142857143\n",
            "Train_ Loss =  1.8574042320251465\n",
            "Train accuracy = 0.638\n",
            "Train_ Loss =  1.883905053138733\n",
            "Eopch = 30\n",
            "Train accuracy = 0.37142857142857144\n",
            "Train_ Loss =  1.8606209754943848\n",
            "Train accuracy = 0.658\n",
            "Train_ Loss =  1.8808046579360962\n",
            "Eopch = 31\n",
            "Train accuracy = 0.4357142857142857\n",
            "Train_ Loss =  1.8170050382614136\n",
            "Train accuracy = 0.66\n",
            "Train_ Loss =  1.8783278465270996\n",
            "Eopch = 32\n",
            "Train accuracy = 0.37857142857142856\n",
            "Train_ Loss =  1.8339948654174805\n",
            "Train accuracy = 0.67\n",
            "Train_ Loss =  1.8754386901855469\n",
            "Eopch = 33\n",
            "Train accuracy = 0.40714285714285714\n",
            "Train_ Loss =  1.8165814876556396\n",
            "Train accuracy = 0.682\n",
            "Train_ Loss =  1.8725254535675049\n",
            "Eopch = 34\n",
            "Train accuracy = 0.37857142857142856\n",
            "Train_ Loss =  1.8343805074691772\n",
            "Train accuracy = 0.696\n",
            "Train_ Loss =  1.8691256046295166\n",
            "Eopch = 35\n",
            "Train accuracy = 0.38571428571428573\n",
            "Train_ Loss =  1.8077565431594849\n",
            "Train accuracy = 0.722\n",
            "Train_ Loss =  1.8650356531143188\n",
            "Eopch = 36\n",
            "Train accuracy = 0.42142857142857143\n",
            "Train_ Loss =  1.805686354637146\n",
            "Train accuracy = 0.742\n",
            "Train_ Loss =  1.860289454460144\n",
            "Eopch = 37\n",
            "Train accuracy = 0.34285714285714286\n",
            "Train_ Loss =  1.827187418937683\n",
            "Train accuracy = 0.774\n",
            "Train_ Loss =  1.8542677164077759\n",
            "Eopch = 38\n",
            "Train accuracy = 0.42142857142857143\n",
            "Train_ Loss =  1.798471450805664\n",
            "Train accuracy = 0.782\n",
            "Train_ Loss =  1.8476482629776\n",
            "Eopch = 39\n",
            "Train accuracy = 0.2857142857142857\n",
            "Train_ Loss =  1.8388224840164185\n",
            "Train accuracy = 0.806\n",
            "Train_ Loss =  1.8407732248306274\n",
            "Eopch = 40\n",
            "Train accuracy = 0.4\n",
            "Train_ Loss =  1.8102253675460815\n",
            "Train accuracy = 0.788\n",
            "Train_ Loss =  1.8329874277114868\n",
            "Eopch = 41\n",
            "Train accuracy = 0.4357142857142857\n",
            "Train_ Loss =  1.7761845588684082\n",
            "Train accuracy = 0.782\n",
            "Train_ Loss =  1.8250418901443481\n",
            "Eopch = 42\n",
            "Train accuracy = 0.3357142857142857\n",
            "Train_ Loss =  1.8139638900756836\n",
            "Train accuracy = 0.778\n",
            "Train_ Loss =  1.8166611194610596\n",
            "Eopch = 43\n",
            "Train accuracy = 0.5357142857142857\n",
            "Train_ Loss =  1.6808007955551147\n",
            "Train accuracy = 0.762\n",
            "Train_ Loss =  1.8094451427459717\n",
            "Eopch = 44\n",
            "Train accuracy = 0.36428571428571427\n",
            "Train_ Loss =  1.792532205581665\n",
            "Train accuracy = 0.748\n",
            "Train_ Loss =  1.8029398918151855\n",
            "Eopch = 45\n",
            "Train accuracy = 0.5071428571428571\n",
            "Train_ Loss =  1.7074989080429077\n",
            "Train accuracy = 0.744\n",
            "Train_ Loss =  1.7968242168426514\n",
            "Eopch = 46\n",
            "Train accuracy = 0.45714285714285713\n",
            "Train_ Loss =  1.7283896207809448\n",
            "Train accuracy = 0.758\n",
            "Train_ Loss =  1.7926188707351685\n",
            "Eopch = 47\n",
            "Train accuracy = 0.45\n",
            "Train_ Loss =  1.7242361307144165\n",
            "Train accuracy = 0.77\n",
            "Train_ Loss =  1.7888356447219849\n",
            "Eopch = 48\n",
            "Train accuracy = 0.45714285714285713\n",
            "Train_ Loss =  1.6801246404647827\n",
            "Train accuracy = 0.784\n",
            "Train_ Loss =  1.7858154773712158\n",
            "Eopch = 49\n",
            "Train accuracy = 0.36428571428571427\n",
            "Train_ Loss =  1.7812604904174805\n",
            "Train accuracy = 0.794\n",
            "Train_ Loss =  1.782704472541809\n",
            "Eopch = 50\n",
            "Train accuracy = 0.44285714285714284\n",
            "Train_ Loss =  1.7574971914291382\n",
            "Train accuracy = 0.816\n",
            "Train_ Loss =  1.7794429063796997\n",
            "Eopch = 51\n",
            "Train accuracy = 0.4714285714285714\n",
            "Train_ Loss =  1.6636582612991333\n",
            "Train accuracy = 0.822\n",
            "Train_ Loss =  1.7764520645141602\n",
            "Eopch = 52\n",
            "Train accuracy = 0.45714285714285713\n",
            "Train_ Loss =  1.6976232528686523\n",
            "Train accuracy = 0.818\n",
            "Train_ Loss =  1.7721853256225586\n",
            "Eopch = 53\n",
            "Train accuracy = 0.4357142857142857\n",
            "Train_ Loss =  1.7415295839309692\n",
            "Train accuracy = 0.808\n",
            "Train_ Loss =  1.7680833339691162\n",
            "Eopch = 54\n",
            "Train accuracy = 0.4\n",
            "Train_ Loss =  1.7207446098327637\n",
            "Train accuracy = 0.808\n",
            "Train_ Loss =  1.7656888961791992\n",
            "Eopch = 55\n",
            "Train accuracy = 0.45714285714285713\n",
            "Train_ Loss =  1.7115144729614258\n",
            "Train accuracy = 0.804\n",
            "Train_ Loss =  1.7641292810440063\n",
            "Eopch = 56\n",
            "Train accuracy = 0.40714285714285714\n",
            "Train_ Loss =  1.7303558588027954\n",
            "Train accuracy = 0.792\n",
            "Train_ Loss =  1.7621575593948364\n",
            "Eopch = 57\n",
            "Train accuracy = 0.42857142857142855\n",
            "Train_ Loss =  1.6615163087844849\n",
            "Train accuracy = 0.784\n",
            "Train_ Loss =  1.7592415809631348\n",
            "Eopch = 58\n",
            "Train accuracy = 0.44285714285714284\n",
            "Train_ Loss =  1.6631559133529663\n",
            "Train accuracy = 0.762\n",
            "Train_ Loss =  1.756715178489685\n",
            "Eopch = 59\n",
            "Train accuracy = 0.44285714285714284\n",
            "Train_ Loss =  1.6862568855285645\n",
            "Train accuracy = 0.738\n",
            "Train_ Loss =  1.7549126148223877\n",
            "Eopch = 60\n",
            "Train accuracy = 0.40714285714285714\n",
            "Train_ Loss =  1.7432068586349487\n",
            "Train accuracy = 0.72\n",
            "Train_ Loss =  1.7520067691802979\n",
            "Eopch = 61\n",
            "Train accuracy = 0.4785714285714286\n",
            "Train_ Loss =  1.6511962413787842\n",
            "Train accuracy = 0.722\n",
            "Train_ Loss =  1.7471771240234375\n",
            "Eopch = 62\n",
            "Train accuracy = 0.45\n",
            "Train_ Loss =  1.719120979309082\n",
            "Train accuracy = 0.726\n",
            "Train_ Loss =  1.7416799068450928\n",
            "Eopch = 63\n",
            "Train accuracy = 0.39285714285714285\n",
            "Train_ Loss =  1.7298572063446045\n",
            "Train accuracy = 0.74\n",
            "Train_ Loss =  1.7348273992538452\n",
            "Eopch = 64\n",
            "Train accuracy = 0.4785714285714286\n",
            "Train_ Loss =  1.639204502105713\n",
            "Train accuracy = 0.748\n",
            "Train_ Loss =  1.7282452583312988\n",
            "Eopch = 65\n",
            "Train accuracy = 0.4714285714285714\n",
            "Train_ Loss =  1.658628225326538\n",
            "Train accuracy = 0.76\n",
            "Train_ Loss =  1.7213841676712036\n",
            "Eopch = 66\n",
            "Train accuracy = 0.4928571428571429\n",
            "Train_ Loss =  1.6016510725021362\n",
            "Train accuracy = 0.762\n",
            "Train_ Loss =  1.7147347927093506\n",
            "Eopch = 67\n",
            "Train accuracy = 0.42142857142857143\n",
            "Train_ Loss =  1.6483045816421509\n",
            "Train accuracy = 0.784\n",
            "Train_ Loss =  1.7086188793182373\n",
            "Eopch = 68\n",
            "Train accuracy = 0.4857142857142857\n",
            "Train_ Loss =  1.6441783905029297\n",
            "Train accuracy = 0.792\n",
            "Train_ Loss =  1.7020089626312256\n",
            "Eopch = 69\n",
            "Train accuracy = 0.4785714285714286\n",
            "Train_ Loss =  1.6021982431411743\n",
            "Train accuracy = 0.79\n",
            "Train_ Loss =  1.6970382928848267\n",
            "Eopch = 70\n",
            "Train accuracy = 0.4142857142857143\n",
            "Train_ Loss =  1.6511669158935547\n",
            "Train accuracy = 0.788\n",
            "Train_ Loss =  1.692007064819336\n",
            "Eopch = 71\n",
            "Train accuracy = 0.44285714285714284\n",
            "Train_ Loss =  1.6654280424118042\n",
            "Train accuracy = 0.8\n",
            "Train_ Loss =  1.6874338388442993\n",
            "Eopch = 72\n",
            "Train accuracy = 0.4357142857142857\n",
            "Train_ Loss =  1.619126796722412\n",
            "Train accuracy = 0.8\n",
            "Train_ Loss =  1.6827749013900757\n",
            "Eopch = 73\n",
            "Train accuracy = 0.4642857142857143\n",
            "Train_ Loss =  1.5762618780136108\n",
            "Train accuracy = 0.806\n",
            "Train_ Loss =  1.6786543130874634\n",
            "Eopch = 74\n",
            "Train accuracy = 0.42857142857142855\n",
            "Train_ Loss =  1.6422412395477295\n",
            "Train accuracy = 0.796\n",
            "Train_ Loss =  1.6753149032592773\n",
            "Eopch = 75\n",
            "Train accuracy = 0.4714285714285714\n",
            "Train_ Loss =  1.5192005634307861\n",
            "Train accuracy = 0.794\n",
            "Train_ Loss =  1.6727368831634521\n",
            "Eopch = 76\n",
            "Train accuracy = 0.45714285714285713\n",
            "Train_ Loss =  1.5933756828308105\n",
            "Train accuracy = 0.788\n",
            "Train_ Loss =  1.6704574823379517\n",
            "Eopch = 77\n",
            "Train accuracy = 0.4857142857142857\n",
            "Train_ Loss =  1.525017499923706\n",
            "Train accuracy = 0.784\n",
            "Train_ Loss =  1.6670386791229248\n",
            "Eopch = 78\n",
            "Train accuracy = 0.4785714285714286\n",
            "Train_ Loss =  1.5799760818481445\n",
            "Train accuracy = 0.782\n",
            "Train_ Loss =  1.6646368503570557\n",
            "Eopch = 79\n",
            "Train accuracy = 0.4785714285714286\n",
            "Train_ Loss =  1.5452556610107422\n",
            "Train accuracy = 0.778\n",
            "Train_ Loss =  1.6637821197509766\n",
            "Eopch = 80\n",
            "Train accuracy = 0.4142857142857143\n",
            "Train_ Loss =  1.648231863975525\n",
            "Train accuracy = 0.778\n",
            "Train_ Loss =  1.6628082990646362\n",
            "Eopch = 81\n",
            "Train accuracy = 0.4857142857142857\n",
            "Train_ Loss =  1.5986515283584595\n",
            "Train accuracy = 0.77\n",
            "Train_ Loss =  1.6624881029129028\n",
            "Eopch = 82\n",
            "Train accuracy = 0.4785714285714286\n",
            "Train_ Loss =  1.5415117740631104\n",
            "Train accuracy = 0.764\n",
            "Train_ Loss =  1.6640164852142334\n",
            "Eopch = 83\n",
            "Train accuracy = 0.4857142857142857\n",
            "Train_ Loss =  1.5288537740707397\n",
            "Train accuracy = 0.756\n",
            "Train_ Loss =  1.6639297008514404\n",
            "Eopch = 84\n",
            "Train accuracy = 0.5\n",
            "Train_ Loss =  1.5291805267333984\n",
            "Train accuracy = 0.754\n",
            "Train_ Loss =  1.6634740829467773\n",
            "Eopch = 85\n",
            "Train accuracy = 0.5142857142857142\n",
            "Train_ Loss =  1.4603126049041748\n",
            "Train accuracy = 0.75\n",
            "Train_ Loss =  1.6608730554580688\n",
            "Eopch = 86\n",
            "Train accuracy = 0.45714285714285713\n",
            "Train_ Loss =  1.553695797920227\n",
            "Train accuracy = 0.752\n",
            "Train_ Loss =  1.6564617156982422\n",
            "Eopch = 87\n",
            "Train accuracy = 0.5357142857142857\n",
            "Train_ Loss =  1.5168704986572266\n",
            "Train accuracy = 0.754\n",
            "Train_ Loss =  1.6521052122116089\n",
            "Eopch = 88\n",
            "Train accuracy = 0.4857142857142857\n",
            "Train_ Loss =  1.4628136157989502\n",
            "Train accuracy = 0.742\n",
            "Train_ Loss =  1.6479078531265259\n",
            "Eopch = 89\n",
            "Train accuracy = 0.5071428571428571\n",
            "Train_ Loss =  1.5365848541259766\n",
            "Train accuracy = 0.738\n",
            "Train_ Loss =  1.6437166929244995\n",
            "Eopch = 90\n",
            "Train accuracy = 0.4785714285714286\n",
            "Train_ Loss =  1.5121158361434937\n",
            "Train accuracy = 0.736\n",
            "Train_ Loss =  1.6405370235443115\n",
            "Eopch = 91\n",
            "Train accuracy = 0.5\n",
            "Train_ Loss =  1.435628890991211\n",
            "Train accuracy = 0.736\n",
            "Train_ Loss =  1.6366583108901978\n",
            "Eopch = 92\n",
            "Train accuracy = 0.42142857142857143\n",
            "Train_ Loss =  1.535807728767395\n",
            "Train accuracy = 0.744\n",
            "Train_ Loss =  1.6322286128997803\n",
            "Eopch = 93\n",
            "Train accuracy = 0.4714285714285714\n",
            "Train_ Loss =  1.5840038061141968\n",
            "Train accuracy = 0.744\n",
            "Train_ Loss =  1.6277203559875488\n",
            "Eopch = 94\n",
            "Train accuracy = 0.4714285714285714\n",
            "Train_ Loss =  1.5056794881820679\n",
            "Train accuracy = 0.766\n",
            "Train_ Loss =  1.6216741800308228\n",
            "Eopch = 95\n",
            "Train accuracy = 0.37857142857142856\n",
            "Train_ Loss =  1.5688350200653076\n",
            "Train accuracy = 0.766\n",
            "Train_ Loss =  1.6152724027633667\n",
            "Eopch = 96\n",
            "Train accuracy = 0.4\n",
            "Train_ Loss =  1.591286301612854\n",
            "Train accuracy = 0.774\n",
            "Train_ Loss =  1.6087974309921265\n",
            "Eopch = 97\n",
            "Train accuracy = 0.4928571428571429\n",
            "Train_ Loss =  1.4077649116516113\n",
            "Train accuracy = 0.772\n",
            "Train_ Loss =  1.6022733449935913\n",
            "Eopch = 98\n",
            "Train accuracy = 0.42857142857142855\n",
            "Train_ Loss =  1.642601490020752\n",
            "Train accuracy = 0.778\n",
            "Train_ Loss =  1.596349835395813\n",
            "Eopch = 99\n",
            "Train accuracy = 0.4857142857142857\n",
            "Train_ Loss =  1.4521406888961792\n",
            "Train accuracy = 0.78\n",
            "Train_ Loss =  1.591517448425293\n",
            "Eopch = 100\n",
            "Train accuracy = 0.45714285714285713\n",
            "Train_ Loss =  1.542994499206543\n",
            "Train accuracy = 0.774\n",
            "Train_ Loss =  1.5881613492965698\n",
            "GAT training: time elapsed= 0.03 [s] | epoch=101 | val acc=0.774\n",
            "Eopch = 101\n",
            "Train accuracy = 0.5071428571428571\n",
            "Train_ Loss =  1.5205034017562866\n",
            "Train accuracy = 0.78\n",
            "Train_ Loss =  1.5833793878555298\n",
            "Eopch = 102\n",
            "Train accuracy = 0.4857142857142857\n",
            "Train_ Loss =  1.4842150211334229\n",
            "Train accuracy = 0.768\n",
            "Train_ Loss =  1.5786547660827637\n",
            "Eopch = 103\n",
            "Train accuracy = 0.4714285714285714\n",
            "Train_ Loss =  1.5428675413131714\n",
            "Train accuracy = 0.754\n",
            "Train_ Loss =  1.5745881795883179\n",
            "Eopch = 104\n",
            "Train accuracy = 0.4642857142857143\n",
            "Train_ Loss =  1.5020700693130493\n",
            "Train accuracy = 0.754\n",
            "Train_ Loss =  1.5690199136734009\n",
            "Eopch = 105\n",
            "Train accuracy = 0.5642857142857143\n",
            "Train_ Loss =  1.4422599077224731\n",
            "Train accuracy = 0.756\n",
            "Train_ Loss =  1.562325119972229\n",
            "Eopch = 106\n",
            "Train accuracy = 0.4857142857142857\n",
            "Train_ Loss =  1.5002357959747314\n",
            "Train accuracy = 0.752\n",
            "Train_ Loss =  1.5556892156600952\n",
            "Eopch = 107\n",
            "Train accuracy = 0.44285714285714284\n",
            "Train_ Loss =  1.5669500827789307\n",
            "Train accuracy = 0.754\n",
            "Train_ Loss =  1.5511953830718994\n",
            "Eopch = 108\n",
            "Train accuracy = 0.5428571428571428\n",
            "Train_ Loss =  1.3964875936508179\n",
            "Train accuracy = 0.778\n",
            "Train_ Loss =  1.546636939048767\n",
            "Eopch = 109\n",
            "Train accuracy = 0.4714285714285714\n",
            "Train_ Loss =  1.4735945463180542\n",
            "Train accuracy = 0.788\n",
            "Train_ Loss =  1.5429422855377197\n",
            "Eopch = 110\n",
            "Train accuracy = 0.5214285714285715\n",
            "Train_ Loss =  1.4425443410873413\n",
            "Train accuracy = 0.794\n",
            "Train_ Loss =  1.5412641763687134\n",
            "Eopch = 111\n",
            "Train accuracy = 0.4857142857142857\n",
            "Train_ Loss =  1.4429954290390015\n",
            "Train accuracy = 0.79\n",
            "Train_ Loss =  1.5405749082565308\n",
            "Eopch = 112\n",
            "Train accuracy = 0.4785714285714286\n",
            "Train_ Loss =  1.4415841102600098\n",
            "Train accuracy = 0.796\n",
            "Train_ Loss =  1.5391106605529785\n",
            "Eopch = 113\n",
            "Train accuracy = 0.5142857142857142\n",
            "Train_ Loss =  1.3831170797348022\n",
            "Train accuracy = 0.79\n",
            "Train_ Loss =  1.5375185012817383\n",
            "Eopch = 114\n",
            "Train accuracy = 0.4714285714285714\n",
            "Train_ Loss =  1.531054139137268\n",
            "Train accuracy = 0.792\n",
            "Train_ Loss =  1.5352818965911865\n",
            "Eopch = 115\n",
            "Train accuracy = 0.6214285714285714\n",
            "Train_ Loss =  1.3069534301757812\n",
            "Train accuracy = 0.802\n",
            "Train_ Loss =  1.532585859298706\n",
            "Eopch = 116\n",
            "Train accuracy = 0.45714285714285713\n",
            "Train_ Loss =  1.5649155378341675\n",
            "Train accuracy = 0.802\n",
            "Train_ Loss =  1.5319563150405884\n",
            "Eopch = 117\n",
            "Train accuracy = 0.5714285714285714\n",
            "Train_ Loss =  1.3795084953308105\n",
            "Train accuracy = 0.8\n",
            "Train_ Loss =  1.5323582887649536\n",
            "Eopch = 118\n",
            "Train accuracy = 0.4357142857142857\n",
            "Train_ Loss =  1.5930931568145752\n",
            "Train accuracy = 0.798\n",
            "Train_ Loss =  1.5349624156951904\n",
            "Eopch = 119\n",
            "Train accuracy = 0.5142857142857142\n",
            "Train_ Loss =  1.4388853311538696\n",
            "Train accuracy = 0.794\n",
            "Train_ Loss =  1.5373194217681885\n",
            "Eopch = 120\n",
            "Train accuracy = 0.5\n",
            "Train_ Loss =  1.4292796850204468\n",
            "Train accuracy = 0.786\n",
            "Train_ Loss =  1.5392091274261475\n",
            "Eopch = 121\n",
            "Train accuracy = 0.4642857142857143\n",
            "Train_ Loss =  1.5195417404174805\n",
            "Train accuracy = 0.784\n",
            "Train_ Loss =  1.5404164791107178\n",
            "Eopch = 122\n",
            "Train accuracy = 0.4785714285714286\n",
            "Train_ Loss =  1.4718210697174072\n",
            "Train accuracy = 0.774\n",
            "Train_ Loss =  1.5411691665649414\n",
            "Eopch = 123\n",
            "Train accuracy = 0.44285714285714284\n",
            "Train_ Loss =  1.4243483543395996\n",
            "Train accuracy = 0.774\n",
            "Train_ Loss =  1.5393484830856323\n",
            "Eopch = 124\n",
            "Train accuracy = 0.4785714285714286\n",
            "Train_ Loss =  1.4187729358673096\n",
            "Train accuracy = 0.776\n",
            "Train_ Loss =  1.5364490747451782\n",
            "Eopch = 125\n",
            "Train accuracy = 0.4642857142857143\n",
            "Train_ Loss =  1.4721276760101318\n",
            "Train accuracy = 0.778\n",
            "Train_ Loss =  1.534257173538208\n",
            "Eopch = 126\n",
            "Train accuracy = 0.4928571428571429\n",
            "Train_ Loss =  1.4042094945907593\n",
            "Train accuracy = 0.776\n",
            "Train_ Loss =  1.5313091278076172\n",
            "Eopch = 127\n",
            "Train accuracy = 0.45\n",
            "Train_ Loss =  1.4941076040267944\n",
            "Train accuracy = 0.774\n",
            "Train_ Loss =  1.52736234664917\n",
            "Eopch = 128\n",
            "Train accuracy = 0.5071428571428571\n",
            "Train_ Loss =  1.4138920307159424\n",
            "Train accuracy = 0.776\n",
            "Train_ Loss =  1.5234251022338867\n",
            "Eopch = 129\n",
            "Train accuracy = 0.4928571428571429\n",
            "Train_ Loss =  1.4190744161605835\n",
            "Train accuracy = 0.776\n",
            "Train_ Loss =  1.5214965343475342\n",
            "Eopch = 130\n",
            "Train accuracy = 0.4857142857142857\n",
            "Train_ Loss =  1.4583300352096558\n",
            "Train accuracy = 0.778\n",
            "Train_ Loss =  1.5202183723449707\n",
            "Eopch = 131\n",
            "Train accuracy = 0.5214285714285715\n",
            "Train_ Loss =  1.4008727073669434\n",
            "Train accuracy = 0.78\n",
            "Train_ Loss =  1.5183136463165283\n",
            "Eopch = 132\n",
            "Train accuracy = 0.44285714285714284\n",
            "Train_ Loss =  1.4300320148468018\n",
            "Train accuracy = 0.78\n",
            "Train_ Loss =  1.516875982284546\n",
            "Eopch = 133\n",
            "Train accuracy = 0.4357142857142857\n",
            "Train_ Loss =  1.4747589826583862\n",
            "Train accuracy = 0.782\n",
            "Train_ Loss =  1.5156581401824951\n",
            "Eopch = 134\n",
            "Train accuracy = 0.4928571428571429\n",
            "Train_ Loss =  1.4070075750350952\n",
            "Train accuracy = 0.784\n",
            "Train_ Loss =  1.5135343074798584\n",
            "Eopch = 135\n",
            "Train accuracy = 0.5214285714285715\n",
            "Train_ Loss =  1.4890470504760742\n",
            "Train accuracy = 0.786\n",
            "Train_ Loss =  1.5105122327804565\n",
            "Eopch = 136\n",
            "Train accuracy = 0.45\n",
            "Train_ Loss =  1.4822378158569336\n",
            "Train accuracy = 0.786\n",
            "Train_ Loss =  1.5104929208755493\n",
            "Eopch = 137\n",
            "Train accuracy = 0.45\n",
            "Train_ Loss =  1.450637698173523\n",
            "Train accuracy = 0.784\n",
            "Train_ Loss =  1.511249303817749\n",
            "Eopch = 138\n",
            "Train accuracy = 0.5785714285714286\n",
            "Train_ Loss =  1.3608930110931396\n",
            "Train accuracy = 0.776\n",
            "Train_ Loss =  1.5120855569839478\n",
            "Eopch = 139\n",
            "Train accuracy = 0.4714285714285714\n",
            "Train_ Loss =  1.4125372171401978\n",
            "Train accuracy = 0.778\n",
            "Train_ Loss =  1.5120453834533691\n",
            "Eopch = 140\n",
            "Train accuracy = 0.5071428571428571\n",
            "Train_ Loss =  1.4686685800552368\n",
            "Train accuracy = 0.77\n",
            "Train_ Loss =  1.5118935108184814\n",
            "Eopch = 141\n",
            "Train accuracy = 0.5214285714285715\n",
            "Train_ Loss =  1.3587206602096558\n",
            "Train accuracy = 0.768\n",
            "Train_ Loss =  1.5111639499664307\n",
            "Eopch = 142\n",
            "Train accuracy = 0.5\n",
            "Train_ Loss =  1.4178704023361206\n",
            "Train accuracy = 0.77\n",
            "Train_ Loss =  1.5076148509979248\n",
            "Eopch = 143\n",
            "Train accuracy = 0.45714285714285713\n",
            "Train_ Loss =  1.4590845108032227\n",
            "Train accuracy = 0.782\n",
            "Train_ Loss =  1.5025715827941895\n",
            "Eopch = 144\n",
            "Train accuracy = 0.5285714285714286\n",
            "Train_ Loss =  1.3641706705093384\n",
            "Train accuracy = 0.788\n",
            "Train_ Loss =  1.4965825080871582\n",
            "Eopch = 145\n",
            "Train accuracy = 0.5928571428571429\n",
            "Train_ Loss =  1.201337218284607\n",
            "Train accuracy = 0.794\n",
            "Train_ Loss =  1.4914634227752686\n",
            "Eopch = 146\n",
            "Train accuracy = 0.5\n",
            "Train_ Loss =  1.3757154941558838\n",
            "Train accuracy = 0.804\n",
            "Train_ Loss =  1.4859563112258911\n",
            "Eopch = 147\n",
            "Train accuracy = 0.4928571428571429\n",
            "Train_ Loss =  1.4898536205291748\n",
            "Train accuracy = 0.808\n",
            "Train_ Loss =  1.4818816184997559\n",
            "Eopch = 148\n",
            "Train accuracy = 0.4857142857142857\n",
            "Train_ Loss =  1.5072294473648071\n",
            "Train accuracy = 0.802\n",
            "Train_ Loss =  1.4803887605667114\n",
            "Eopch = 149\n",
            "Train accuracy = 0.5357142857142857\n",
            "Train_ Loss =  1.3292211294174194\n",
            "Train accuracy = 0.8\n",
            "Train_ Loss =  1.4805562496185303\n",
            "Eopch = 150\n",
            "Train accuracy = 0.5071428571428571\n",
            "Train_ Loss =  1.374316692352295\n",
            "Train accuracy = 0.8\n",
            "Train_ Loss =  1.4828667640686035\n",
            "Eopch = 151\n",
            "Train accuracy = 0.44285714285714284\n",
            "Train_ Loss =  1.5127856731414795\n",
            "Train accuracy = 0.798\n",
            "Train_ Loss =  1.482635736465454\n",
            "Eopch = 152\n",
            "Train accuracy = 0.4785714285714286\n",
            "Train_ Loss =  1.428027868270874\n",
            "Train accuracy = 0.8\n",
            "Train_ Loss =  1.48049795627594\n",
            "Eopch = 153\n",
            "Train accuracy = 0.5\n",
            "Train_ Loss =  1.4654645919799805\n",
            "Train accuracy = 0.798\n",
            "Train_ Loss =  1.477663278579712\n",
            "Eopch = 154\n",
            "Train accuracy = 0.5142857142857142\n",
            "Train_ Loss =  1.4782568216323853\n",
            "Train accuracy = 0.81\n",
            "Train_ Loss =  1.474462866783142\n",
            "Eopch = 155\n",
            "Train accuracy = 0.5428571428571428\n",
            "Train_ Loss =  1.338626742362976\n",
            "Train accuracy = 0.816\n",
            "Train_ Loss =  1.470249056816101\n",
            "Eopch = 156\n",
            "Train accuracy = 0.4642857142857143\n",
            "Train_ Loss =  1.4729033708572388\n",
            "Train accuracy = 0.812\n",
            "Train_ Loss =  1.4641568660736084\n",
            "Eopch = 157\n",
            "Train accuracy = 0.44285714285714284\n",
            "Train_ Loss =  1.5258618593215942\n",
            "Train accuracy = 0.81\n",
            "Train_ Loss =  1.4615216255187988\n",
            "Eopch = 158\n",
            "Train accuracy = 0.5\n",
            "Train_ Loss =  1.3573904037475586\n",
            "Train accuracy = 0.802\n",
            "Train_ Loss =  1.460500955581665\n",
            "Eopch = 159\n",
            "Train accuracy = 0.5\n",
            "Train_ Loss =  1.4339168071746826\n",
            "Train accuracy = 0.804\n",
            "Train_ Loss =  1.45943021774292\n",
            "Eopch = 160\n",
            "Train accuracy = 0.5214285714285715\n",
            "Train_ Loss =  1.3665776252746582\n",
            "Train accuracy = 0.81\n",
            "Train_ Loss =  1.4584660530090332\n",
            "Eopch = 161\n",
            "Train accuracy = 0.4785714285714286\n",
            "Train_ Loss =  1.4599032402038574\n",
            "Train accuracy = 0.81\n",
            "Train_ Loss =  1.460721492767334\n",
            "Eopch = 162\n",
            "Train accuracy = 0.5\n",
            "Train_ Loss =  1.3812686204910278\n",
            "Train accuracy = 0.806\n",
            "Train_ Loss =  1.4616731405258179\n",
            "Eopch = 163\n",
            "Train accuracy = 0.5714285714285714\n",
            "Train_ Loss =  1.2728239297866821\n",
            "Train accuracy = 0.81\n",
            "Train_ Loss =  1.4619545936584473\n",
            "Eopch = 164\n",
            "Train accuracy = 0.4857142857142857\n",
            "Train_ Loss =  1.3721307516098022\n",
            "Train accuracy = 0.804\n",
            "Train_ Loss =  1.4628515243530273\n",
            "Eopch = 165\n",
            "Train accuracy = 0.5214285714285715\n",
            "Train_ Loss =  1.3460099697113037\n",
            "Train accuracy = 0.81\n",
            "Train_ Loss =  1.4633547067642212\n",
            "Eopch = 166\n",
            "Train accuracy = 0.5285714285714286\n",
            "Train_ Loss =  1.3065381050109863\n",
            "Train accuracy = 0.816\n",
            "Train_ Loss =  1.4618233442306519\n",
            "Eopch = 167\n",
            "Train accuracy = 0.5285714285714286\n",
            "Train_ Loss =  1.2855260372161865\n",
            "Train accuracy = 0.816\n",
            "Train_ Loss =  1.4605200290679932\n",
            "Eopch = 168\n",
            "Train accuracy = 0.44285714285714284\n",
            "Train_ Loss =  1.5318653583526611\n",
            "Train accuracy = 0.81\n",
            "Train_ Loss =  1.459357500076294\n",
            "Eopch = 169\n",
            "Train accuracy = 0.4857142857142857\n",
            "Train_ Loss =  1.4337239265441895\n",
            "Train accuracy = 0.808\n",
            "Train_ Loss =  1.4576363563537598\n",
            "Eopch = 170\n",
            "Train accuracy = 0.5285714285714286\n",
            "Train_ Loss =  1.3898547887802124\n",
            "Train accuracy = 0.81\n",
            "Train_ Loss =  1.4560505151748657\n",
            "Eopch = 171\n",
            "Train accuracy = 0.4928571428571429\n",
            "Train_ Loss =  1.4203836917877197\n",
            "Train accuracy = 0.81\n",
            "Train_ Loss =  1.4532302618026733\n",
            "Eopch = 172\n",
            "Train accuracy = 0.4785714285714286\n",
            "Train_ Loss =  1.4446099996566772\n",
            "Train accuracy = 0.808\n",
            "Train_ Loss =  1.4509644508361816\n",
            "Eopch = 173\n",
            "Train accuracy = 0.4857142857142857\n",
            "Train_ Loss =  1.3757798671722412\n",
            "Train accuracy = 0.802\n",
            "Train_ Loss =  1.4480139017105103\n",
            "Eopch = 174\n",
            "Train accuracy = 0.5142857142857142\n",
            "Train_ Loss =  1.4083292484283447\n",
            "Train accuracy = 0.802\n",
            "Train_ Loss =  1.4454506635665894\n",
            "Eopch = 175\n",
            "Train accuracy = 0.4928571428571429\n",
            "Train_ Loss =  1.3992592096328735\n",
            "Train accuracy = 0.802\n",
            "Train_ Loss =  1.4430689811706543\n",
            "Eopch = 176\n",
            "Train accuracy = 0.4642857142857143\n",
            "Train_ Loss =  1.373787760734558\n",
            "Train accuracy = 0.806\n",
            "Train_ Loss =  1.4406968355178833\n",
            "Eopch = 177\n",
            "Train accuracy = 0.42857142857142855\n",
            "Train_ Loss =  1.454423189163208\n",
            "Train accuracy = 0.81\n",
            "Train_ Loss =  1.4379081726074219\n",
            "Eopch = 178\n",
            "Train accuracy = 0.55\n",
            "Train_ Loss =  1.4255033731460571\n",
            "Train accuracy = 0.814\n",
            "Train_ Loss =  1.4360511302947998\n",
            "Eopch = 179\n",
            "Train accuracy = 0.4714285714285714\n",
            "Train_ Loss =  1.416620135307312\n",
            "Train accuracy = 0.812\n",
            "Train_ Loss =  1.432563066482544\n",
            "Eopch = 180\n",
            "Train accuracy = 0.4857142857142857\n",
            "Train_ Loss =  1.3926334381103516\n",
            "Train accuracy = 0.816\n",
            "Train_ Loss =  1.430955171585083\n",
            "Eopch = 181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xCL19TgdQ4Ou"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
